---
title: "Project Scratchpad"
author:
  - name: Matthew J. C. Crump
    orcid: 0000-0002-5612-0090
    email: mcrump@brooklyn.cuny.edu
    affiliations:
      - name: Brooklyn College of CUNY
description: "Scratchpad for thinking through the project."
date: 6-27-2023
title-block-style: default
order: 1
---

I've already started on the work in a blog post: <https://crumplab.com/blog/771_GPT_Stroop/>

Need to briefly list what I've done already.

Then, come up with some goals for moving the project forward.

## Blog post summary

- Briefly described the Stroop task
- Develops some motivation for why I care whether or not LLMs can simulate performance in a Stroop-task
  - data-spoofing, such as mturk workers using LLMs to exploit online tasks.
  
- Describe methods
  - use the openai API and R to send instructions for a Stroop task, and then individual trials (in text format)
  - have the model simulate trial-by-trial responses and reaction times, return them in JSON
  - analyse the data and see what happens
  
- The post shows some draft code to simulate a single subject, and to simulate multiple subjects

- Got data from 10 simulated subjects

- Results showed:
  - gpt-3.5-turbo generated data files that showed Stroop effects
  - Simulated RTs were different across simulated subjects
  - Accuracy was 100%
  - RTs looked almost credible. Many individual RTs had 0 ending, and were too round looking.

## To do

Not an exhaustive list, something to get me started.

- [x] Settle on one script that can be extended across the examples. 
- [x] Run multiple subjects, say batches of 20-30, which should be enough for the kinds of questions I want to ask

Answer the following basic questions

- [x] Does the model produce Stroop effects in RT and accuracy?
- [x] Does the model produce different answers for each run of simulated subjects?
- [x] Does the model produce RTs that look like human subject RTs?
- [x] Does the model produce additional Stroop phenomena without further prompting?
  - [x] Congruency sequence effect?
  - [] Proportion Congruent effect?
- [x] Can the instruction prompt be used to control how the model simulates performance.
  - [x] simulate 75% correct
  - [] simulate 50% correct
  - [] simulate some long reaction times
  - [] simulate a reverse Stroop effect.
  - [] simulate proportion congruent effects

## Simulations

### Simulation 1: Basic Stroop

25 simulated subjects. 24 trials each. 50% congruent/incongruent. Simple instruction prompt. Shows Stroop effects, and congruent sequence effects. RTs are too round. Accuracy is too good.

### Simulation 2: Stroop Instructions

Same as above, but change to prompt for more "random" RTs, and worse accuracy. This worked.

### Simulation 3: Proportion congruent

Give the model a low or high proportion congruent list. Don't say anything about high or low proportion congruent being an issue in the prompt. See if the model generates data that nevertheless show a proportion congruent effect.

### Simulation 4: Reverse the task

Have the model perform "word-reading" instead of "color-naming". This usually shows a much smaller Stroop. What does the model do?









