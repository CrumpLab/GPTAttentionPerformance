---
title: "Interim Overview"
author:
  - name: Matthew J. C. Crump
    orcid: 0000-0002-5612-0090
    email: mcrump@brooklyn.cuny.edu
    affiliations:
      - name: Brooklyn College of CUNY
abstract: "An iterim overview of this project up to simulation 8."
date: 6-28-2023
title-block-style: default
order: 1
format:
  html:
    code-overflow: wrap
    code-fold: true
    code-summary: "Show the code"
---

::: callout-tip
This project and this summary are draft in progress works that may change at any time until this project is completed.
:::

## Motivation: Why am I doing this?

I'm not prepared to flesh this section out today, so it is a promise for the future. I developed some of the motivation for this project in a blog post:

<https://crumplab.com/blog/771_GPT_Stroop/>

Elements of the motivation include:

-   Personal curiosity, and an excuse to try programmatically interacting with the OpenAI API.

-   General concerns about LLMs being used to spoof human data in online situations, such as recent reports that mturk workers are using LLMs to respond to HITs. I've noticed similar behavior in some of my own mturk requests.

-   Wondering whether an LLM could fake data in the tasks I typically run online. These tasks are usually programmed in JsPsych, and require speeded identification responses on a trial-by-trial basis.

## What is this project?

In this project I am asking general questions about whether large language models, such as OpenAI's gpt-3.5-turbo, can spoof human-like data in attention and performance tasks. The simulations so far have employed a text-based version of the Stroop task.

The figure below shows example Stroop stimuli that might be presented to people. The task is typically to name the ink-color of the word, and avoid naming the actual word. Responses are typically faster for congruent items-- where the color matches the word-- than incongruent item types-- where the color mismatches the word.

![](https://crumplab.com/blog/771_GPT_Stroop/Stroop.png)

The general question for this project was, what happens if I ask a GPT model to perform a Stroop task?

I'm using a number of tools that greatly facilitate my ability to ask this kind of question. These include, R, an R library for interfacing with the OpenAI API, and the ability of GPT models to return their responses in JSON (most of the time), which makes it easy to analyse simulated responses in R.

The details of the simulations run so far can be found on their respective pages in the modeling section. The approach is basically to prompt the model that it is about to perform a Stroop task. I provide trial-by-trial text saying what kind of item is presented on each trial (e.g., the word blue is written in the color red), and I ask the model to identify the color, and provide a simulated reaction time in milliseconds.

## What happened so far?

The TLDR is that GPT can spoof Stroop data in several ways, and even in some interesting ways. The prompts I used have tell-tale signs that the data was generated by an LLM. Sometimes prompt modifications change the simulated behavior, and sometimes they did not. Let's review the simulations. All of the details for each simulation can be found in their respective pages in the modeling section.

## Simulation 1: Basic Stroop 

This simulation used a prompt like:

------------------------------------------------------------------------

Consider the following trials of a Stroop task where you are supposed to identify the ink-color of the word as quickly and accurately as possible:

The word red printed in the color blue

The word red printed in the color green

The word red printed in the color yellow

...etc. for 24 trials.

This is a simulated Stroop task. You will be shown a Stroop item in the form of a sentence. The sentence will describe a word presented in a particular ink-color. Your task is to identify the ink-color of the word as quickly and accurately as possible. Put the simulated identification response and reaction time into a JSON array using this format: \[{"trial": "trial number, integer", "word": "the name of the word, string","color": "the color of the word, string","response": "the simulated identification response, string","reaction_time": "the simulated reaction time, milliseconds an integer"}\].

------------------------------------------------------------------------

The model produced valid JSON files that I could read into R 23 out of 25 times. The simulated data is in the form of a data frame with responses and reaction times for every trial, for each simulated subject. I read the simulated data into R, then analysed the data to determine whether or not the LLM would produce a Stroop effect. Specifically, would it generate fake reaction times that were on average faster for congruent than incongruent trials?

The answer is yes, these are the results from the model. In addition to simulating faster mean reaction times for congruent than incongruent items, it also produced different results for each simulated subject. These aspects of the data were not explicitly prompted, or at least I didn't think I explicitly prompted them.

![](https://crumplab.com/GPTAttentionPerformance/modeling/S1_basic_stroop_files/figure-html/unnamed-chunk-5-1.png)

The model also generated really unrealistic numbers for reaction times at the level of individual trials. Almost all of the numbers were too round, they ended in 0 or 5, mostly 0. Also, the model was 100% accurate on all trials.

A curious finding was that the model also produced a congruency sequence effect. A common finding in the Stroop literature is that Stroop effects are larger following congruent trials than incongruent trials. Information about this phenomena was not included in the prompt. Nevertheless, when I analyzed the data for the effect, it showed up:

![](https://crumplab.com/GPTAttentionPerformance/modeling/S1_basic_stroop_files/figure-html/unnamed-chunk-10-1.png)

It's possible this is a spurious finding, I don't know. The lack of compuational reproducibility in this kind of modeling work is very frustrating. It's possible that the model was trained on raw data from human Stroop experiments, so when it produced data for this prompt it used patterns that were available from the training set to generate simulated data. Or is that possible? I don't know enough about how these models work to answer the question.

## Simulation 2: Stroop instructions

This simulation includes some additional encouragement in the prompt to generate more realistic reaction times (not always ending in 0), and to make mistakes on some trials. Otherwise, it was similar to the first simulation. Again, it produces a Stroop effect with inter and intra-individual variability in reaction times.

![](https://crumplab.com/GPTAttentionPerformance/modeling/S2_stroop_instructions_files/figure-html/unnamed-chunk-5-1.png)

This time the reaction times did not all in 0, and collectively they even looked similar to human reaction time distributions (see simulation 2). The accuracy data was perfect for congruent trials, but mistakes were inserted on incongruent trials.

The take-home message was that changing the prompt did cause the model to produce reaction times and accuracy rates that conformed to the instructions.
